<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.13"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>CLASSIC: Preparing a CLASSIC run</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript">
  $(document).ready(initResizable);
</script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">CLASSIC
   </div>
   <div id="projectbrief">Canadian Land Surface Scheme including Biogeochemical Cycles</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.13 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
$(document).ready(function(){initNavTree('runPrep.html','');});
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="headertitle">
<div class="title">Preparing a CLASSIC run </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><ol type="1">
<li><a class="el" href="runPrep.html#Environ">Setting up the runtime environment</a><ol type="a">
<li><a class="el" href="runPrep.html#Containers">Running CLASSIC in a Singularity Container</a></li>
</ol>
</li>
<li><a class="el" href="runPrep.html#compilingMod">Compiling CLASSIC for serial and parallel simulations</a></li>
<li><a class="el" href="runPrep.html#setupJobOpts">Setting up the joboptions file</a></li>
<li><a class="el" href="xmlSystem.html">Configuring the model outputs via the CLASSIC code and Output Variable Editor (OVE)</a> <hr/>
</li>
</ol>
<h1><a class="anchor" id="Environ"></a>
Setting up the runtime environment</h1>
<p>To run CLASSIC you can either use your own immediate environment or use our Singularity container (described below). If you use your own immediate environment, the following libraries are required at a minimum:</p>
<ul>
<li>make</li>
<li>libnetcdff-dev</li>
<li>git</li>
<li>gfortran</li>
<li>netcdf-bin</li>
<li>zlib1g</li>
</ul>
<p>These libraries will allow serial compiling and running of the model. To run in parallel add mpich. To run the documentation tool add doxygen.</p>
<h1><a class="anchor" id="Containers"></a>
Running CLASSIC in a Singularity Container</h1>
<h2>What are containers?</h2>
<p>Containers are a tool to package and distribute all the elements and dependencies of a Linux-based application. Within a container it is possible to store the computing environment along with applications such as model code. Containers bring an ease of transportation, installation, and execution across operating systems such as Linux (local or cloud), Mac, and Windows. <a href="https://www.docker.com/">Docker is a type of container</a>. <a href="https://www.sylabs.io/">Singularity</a> is a platform that will allow us to build, run or shell into one of these Docker containers.</p>
<p>In order to run CLASSIC, we need several specific software tools such as compilers (e.g. GNU, Intel, Cray, etc.) and libraries (MPI, NetCDF etc.) that have to be present on our machine to be able to run the model. If we use a container then we can simply access the model through the container, a process that eliminates the cumbersome and time consuming process of installing all of these compilers and libraries locally.</p>
<p>Another significant advantage is that the versions of each library is "frozen" in the container. This has several advantages for scientific reproducibility, mobility, and model development. For example, we could have a container with the library versions at the time of a major model release version. So, even several years later, we can run the model with the original intended library versions recreating the software environment exactly. A version of the WRF weather prediction model has been containerized to aid in it use in teaching and research (Hacker et al. 2016) <a class="el" href="citelist.html#CITEREF_Hacker2016-qg">[42]</a> .</p>
<h2>Benefits of {Singularity} containers</h2>
<p>From Kurtzer et al. (2017) <a class="el" href="citelist.html#CITEREF_Kurtzer2017-xc">[51]</a> :</p>
<blockquote class="doxtable">
<p>Singularity offers mobility of compute by enabling environments to be completely portable via a single image file, and is designed with the features necessary to allow seamless integration with any scientific computational resources. ... Mobility of compute is defined as the ability to define, create, and maintain a workflow locally while remaining confident that the workflow can be executed on different hosts, Linux operating systems, and/or cloud service providers. In essence, mobility of compute means being able to contain the entire software stack, from data files up through the library stack, and reliability move it from system to system. Mobility of compute is an essential building block for reproducible science, and consistent and continuous deployment of applications. ... Many of the same features that facilitate mobility also facilitate reproducibility. Once a contained workflow has been defined, the container image can be snapshotted, archived, and locked down such that it can be used later and the user can be confident that the code within the container has not changed. The container is not subject to any external influence from the host operating system (aside from the kernel which is ubiquitous of any OS level virtualization solution).... Singularity can give the user the freedom they need to install the applications, versions, and dependencies for their workflows without impacting the system in any way. Users can define their own working environment and literally copy that environment image (a single file) to a shared resource, and run their workflow inside that image. </p>
</blockquote>
<p>A nice benefit of containers is that they are designed to be easy to use <a class="el" href="citelist.html#CITEREF_Kurtzer2017-xc">[51]</a> :</p>
<blockquote class="doxtable">
<p>The goal of Singularity is to support existing and traditional HPC resources as easily as installing a single package onto the host operating system. For the administrators of the hosts, some configuration may be required via a single configuration file, however the default values are tuned to be generally applicable for shared environments. </p>
</blockquote>
<p>System administrators will be comforted to know <a class="el" href="citelist.html#CITEREF_Kurtzer2017-xc">[51]</a> :</p>
<blockquote class="doxtable">
<p>Singularity does not provide a pathway for privilege escalation (which makes it truly applicable for multi-tenant shared scientific compute resources). This means that in the runtime environment, a user inside a Singularity container is the same user as outside the container. If a user wants to be root inside the container, they must first become root outside the container. Considering on most shared resources the user will not have root access means they will not have root access within their containers either. This simple concept thus defines the Singularity usage workflow. </p>
</blockquote>
<h2>How to use Singularity containers?</h2>
<p>In order to use Singularity containers, one must first make certain that a local installation of Singularity is available.</p>
<p>For most up-to-date instructions on installing Singularity on Linux, Mac, or Windows see the <a href="https://www.sylabs.io/docs/">Singularity documentation</a>.</p>
<p>Generally, on a Linux machine (Ubuntu in our particular case), one may use aptitude with the following command to install singularity (providing the user has administrative privelidges).</p>
<p><code>sudo apt install singularity-container</code></p>
<h2>Obtaining the CLASSIC Singularity container</h2>
<p>Download our container:</p>
<p><code>singularity pull shub://jormelton/containerCLASSIC</code></p>
<p>And shell into it (example assumes you are in the same folder as the .simg file):</p>
<p><code>singularity shell jormelton-containerCLASSIC-master-latest.simg</code></p>
<p>E.g. </p><pre class="fragment">    acrnrjm@cccsing: ~&gt; singularity shell /user/nphome1/rjm/jormelton-containerCLASSIC-master-latest.simg
    Singularity: Invoking an interactive shell within container...

    Singularity jormelton-containerCLASSIC-master-latest.simg:~&gt;
</pre><p>If that is successful, you are now in the CLASSIC container environment. This environment contains all the libraries needed to run the model (Note this is a bare-bones installation with only the run-time environment. It does not presently contain a workflow or compiled model code).</p>
<p>E.g. test if gfortran is installed: </p><pre class="fragment">    Singularity jormelton-containerCLASSIC-master-latest.simg:~&gt; gfortran
    gfortran: fatal error: no input files
    compilation terminated.
    Singularity jormelton-containerCLASSIC-master-latest.simg:~&gt;
</pre><p>And test for something that is not installed: </p><pre class="fragment">    Singularity jormelton-containerCLASSIC-master-latest.simg:~&gt; okular
    bash: okular: command not found
    Singularity jormelton-containerCLASSIC-master-latest.simg:~&gt;
</pre><p>You can now navigate to the location of CLASSIC code, compile and run the model.</p>
<p>E.g. </p><pre class="fragment">    Singularity jormelton-containerCLASSIC-master-latest.simg:~/Documents/CLASSIC&gt; bin/CLASSIC
     Usage is as follows

     bin/CLASSIC joboptions_file longitude/{longitude}/latitude/{latitude}

     - joboptions_file - an example is
       configurationFiles/template_job_options_file.txt.

     - longitude/latitude
       e.g. 105.23/40.91

      *OR*
      if you wish to run a region then you give
      the corners of the box you wish to run

     - longitude/longitude/latitude/latitude
       e.g. 90/105/30/45
</pre><p>One thing to note: once within the container you will not be able to 'see' remote servers since your container environment is not aware of them. So pay attention to where your input and output files are located.</p>
<h1><a class="anchor" id="compilingMod"></a>
Compiling CLASSIC for serial and parallel simulations</h1>
<p>CLASSIC's Makefile (/Makefile) is setup to allow easy compilation for serial or parallel model running.</p>
<p>There are three options when compiling:</p>
<ul>
<li>For a serial run, use the command "make mode=serial" or just "make"</li>
<li>For a parallel run, use either the command "make mode=parallel"</li>
<li>For a supercomputer run, use the command "make mode=supercomputer"</li>
</ul>
<p>The serial compilation is presently set to use the GNU compiler (gfortran). Parallel compilation uses the GNU MPI compiler mpif90. The supercomputer compilation is setup for the ECCC supercomputer environment and is unlikely to be useful for non-ECCC users.</p>
<p>Upon compilation all object (.o) and module (.mod) files are placed in the objectFiles folder. The executable in placed in the bin folder.</p>
<p>A useful command is 'make clean', which removes all *.o *.mod and the model binary. This can allow a fresh compilation which can be handy if some parameters are changed that aren't being refreshed on a make.</p>
<h1><a class="anchor" id="setupJobOpts"></a>
Setting up the joboptions file</h1>
<p>The joboptions file controls the model configuration, inputs files used, and model outputs. The template joboptions file is located in the configurationFiles folder. Use this as your starting point.</p>
<p>If, for example, we wanted to run CLASSIC at a site with observed meteorology, with leap years, from 1991 to 2017 then we can set up the meteorological options as follows: </p><pre class="fragment">    &amp;joboptions

    ! Meteorological options:
        readMetStartYear = 1991  !&lt; First year of meteorological forcing to read in from the met file
        readMetEndYear = 2017    !&lt; Last year of meteorological forcing to read in from the met file
        metLoop = 1 ,            !&lt; no. of times to cycle over the read-in meteorology
        leap = .true. ,         !&lt; True if your meteorological forcing includes leap years
</pre><p>If we are interested in spinning up the C pools, we could set metLoop to run the model over the readMetStartYear to readMetEndYear a metLoop number of times. We should also point to our meteorological forcing files like, </p><pre class="fragment">    ! Meteorological forcing files:
        metFileFss = pathToFile/dswrf.nc',        !&lt; location of the incoming shortwave radiation meteorology file
        metFileFdl = 'pathToFile/dlwrf.nc',        !&lt; location of the incoming longwave radiation meteorology file
        metFilePre = 'pathToFile/pre.nc',        !&lt; location of the precipitation meteorology file
        metFileTa = 'pathToFile/tmp.nc',         !&lt; location of the air temperature meteorology file
        metFileQa = 'pathToFile/spfh.nc',         !&lt; location of the specific humidity meteorology file
        metFileUv = 'pathToFile/wind.nc',         !&lt; location of the wind speed meteorology file
        metFilePres = 'pathToFile/pres.nc',       !&lt; location of the atmospheric pressure meteorology file
</pre><p>The model initialization and restart files need to be pointed to. Note the rs_file_to_overwrite is <b>overwritten</b>. The simplest thing to do at a start of a run is to make a copy of the init_file to be the rs_file_to_overwrite. CLASSIC only overwrites the prognostic variables leaving any others unchanged (see <a class="el" href="namespacemodel__state__drivers.html#a0864e6b6afaeae951a26d12fef4d2f42" title="Write out the model restart file to netcdf. We only write out the variables that the model influences...">model_state_drivers.write_restart</a>). The model parameters file also needs to be pointed to. </p><pre class="fragment">    ! Initialization and restart files
        init_file = 'inputFiles/CLASSCTEM_initialization_bulkdetrital_nosnow.nc' ,     !&lt; location of the model initialization file
        rs_file_to_overwrite = 'rsFile.nc' ,       !&lt; location of the existing netcdf file that will be **overwritten** for the restart file
                                                   !! typically here you will just copy the init_file and rename it so it can be overwritten.
    ! Namelist of model parameters
        runparams_file = 'configurationFiles/template_run_parameters.txt' ,    !&lt; location of the namelist file containing the model parameters
</pre><p>The next series of switches relate to CTEM. If you wish to run a physics only run with prescribed vegetation parameters, ctem_on is set to .false. and the remainder of this section is ignored. The physics switches are farther down. </p><pre class="fragment">    ! CTEM (biogeochemistry) switches:
     ctem_on = .true. ,     !&lt; set this to true for using ctem simulated dynamic lai and canopy mass, else class simulated specified
                            !&lt; lai and canopy mass are used. with this switch on, all the main ctem subroutines are run.
        icc = 9 ,           !&lt; Number of CTEM level PFTs. NOTE: The number specified here must match the data in your init netcdf file.
        l2max = 3 ,         !&lt; Maximum number of level 2 CTEM PFTs. This is the maximum number of CTEM PFTs associated with a single CLASS PFT.
        spinfast = 3 ,      !&lt; Set this to a higher number up to 10 to spin up soil carbon pool faster. Set to 1 for final round of spin up and transient runs.
</pre><p>The CO2 and CH4 switches behave similarly. If a constant [CO2] is desired, transientCO2 is set to .false. and the year of observed CO2 is specified in fixedYearCO2. The [CO2] of the corresponding year is selected from the CO2File and used for the run. Simlarly for CH4. If are interested in simulating methane related variables then simply copy your CO2File to be your CH4file (e.g. cp CO2file.nc fakeCH4file.nc, and then specify the fakeCH4file.nc for the CH4File since the model expects a file there). If for some reason you wish to run with non-historical [CO2], you could edit your CO2File (using a combination of the NCO tools ncdump and ncgen, for example) to include a future year with associated CO2 value (like 2050 and 500ppm for example) </p><pre class="fragment">    !CO2 switches:
        transientCO2 = .true. , !&lt; Read in time varying CO2 concentration from CO2File or if set to false then use only the year of fixedYearCO2 value
        CO2File = 'inputFiles/mole_fraction_of_carbon_dioxide_in_air_input4MIPs_GHGConcentrations_CMIP_UoM-CMIP-1-2-0_gr1-GMNHSH_1850-2014_absTimeIndex.nc' ,
        fixedYearCO2 = 1850 ,   !&lt; If transientCO2 = .true., this is ignored.

    !CH4 switches:
        ! If you don't care about running with methane, you can just copy your CO2 file (cp CO2file.nc fakeCH4file.nc) and point to it here.
        ! It won't affect the rest of your run. However you do need to have a file specified if CTEM is on.
        transientCH4 = .true. , !&lt; Read in time varying CH4 concentration from CH4File or if set to false then use only the year of fixedYearCH4 value
        CH4File = 'inputFiles/mole_fraction_of_methane_in_air_input4MIPs_GHGConcentrations_CMIP_UoM-CMIP-1-2-0_gr1-GMNHSH_1850-2014_absTimeAxis.nc',
        fixedYearCH4 = 1850 ,   !If transientCH4 = .true., this is ignored.
</pre><p>Disturbance in the form of fire is optional in CLASSIC. If fire is turned on then the model requires a population density input file (see <a class="el" href="CTEMaddInputs.html#initPopd">Population density for fire ignition/suppresion</a>) which is used in a manner similar to CO2 and CH4, i.e. it can use a fixed or transient value. Lightning strikes (see <a class="el" href="CTEMaddInputs.html#initLightFire">Lightning frequency for fire ignition</a>) are also required and can be specified similarly to population density. </p><pre class="fragment">    !Fire switches
        dofire = .true. ,               !&lt; If true the fire disturbance parameterization is turned on.

            transientPOPD = .true. ,    !&lt; Read in time varying population density from POPDFile or if set to false then use only the year of fixedYearPOPD.
            POPDFile = 'inputFiles/POPD_annual_1850_2016_T63_Sep_2017_chunked.nc' ,
            fixedYearPOPD = 1850 ,      !&lt; If transientPOPD = .true., this is ignored.

            transientLGHT= .true.      !&lt; use lightning strike time series, otherwise use fixedYearLGHT
            LGHTFile = 'inputFiles/lisotd_1995_2014_climtlgl_lghtng_as_ts_1850_2050_chunked.nc' , !&lt; Location of the netcdf file containing lightning strike values
            fixedYearLGHT = 1850 ,    !&lt; set the year to use for lightning strikes if transientLGHT is false.
</pre><p>Competition for space between plant functional types is parameterized in CLASSIC. If PFTCompetition is set to false, the PFT fractional coverage follows rules as will be outlined next. If PFTCompetition is true then CLASSIC has two more switches of interest. Competition uses bioclimatic indices to determine whether PFTs should be allowed to compete within a gridcell (see <a class="el" href="namespacecompetition__scheme.html#a4345a1807f52b8da4f0cdd9f0f71f91f" title="Determines if a PFT can exist in a grid cell based on climatic conditions. ">competition_scheme.existence</a> and <a class="el" href="CTEMaddInputs.html#initClimComp">Climatic variables for PFT competition simulations</a>). The bioclimatic indices are either read-in from the init_file or are calculated anew for the run underway. If the values are in the init_file from a spinup run then set inibioclim to true, otherwise set inibioclim to false. It is also possible to start the model from bare ground (rather than the PFT configuration found in your init_file) by setting start_bare to true. </p><pre class="fragment">    ! Competition switches:
        PFTCompetition = .false. ,      !&lt; If true, competition between PFTs for space on a grid cell is implimented
            inibioclim = .false. ,      !&lt; set this to true if competition between pfts is to be implimented and you have the mean climate values
                                        !&lt; in the init netcdf file.
            start_bare = .false.,       !&lt; Set this to true if competition is true, and if you wish to start from bare ground. if this is set to false, the
                                        !&lt; init netcdf file info will be used to set up the run. NOTE: This still keeps the crop fractions
                                        !&lt; (while setting all pools to zero)
</pre><p>Land use change is possible via a LUCFile (see <a class="el" href="CTEMaddInputs.html#inputLUC">Land use change (LUC)</a>) that has the fractional coverage for each PFT annually. This switch has an additional option as described in the comment below. Pay attention here. </p><pre class="fragment">    ! Land Use switches:
        !** If you wish to use your own PFT fractional covers (specified in the init_file), set fixedYearLUC to -9999, otherwise set it
        ! to the year of land cover you want to use. If you wish to have transient land cover changes, set
        ! lnduseon to true, it will update the fractional coverages from LUCFile. When lnduseon is false it is
        ! not updated beyond the initial read in of landcover for fixedYearLUC, or if -9999 then the LUCFile is
        ! not used at all.**
        lnduseon = .true. ,
        LUCFile = 'inputFiles/LUH_HYDE_based_crop_area_adjusted_land_cover_CTEM_fractions_1850_2017_T63_chunked.nc' ,
        fixedYearLUC = 1901 ,
</pre><p>CLASSIC can determine dynamics wetland locations for wetland methane emissions. Alternatively CLASSIC can read in time evolving wetland fractions from an external file. </p><pre class="fragment">    ! Wetland switches:
        ! If you wish to read in and use observed wetland fractions, there are two options. If you wish time
        ! evolving wetland fractions set transientOBSWETF to true and give a OBSWETFFile. If you wish to use
        ! a single year of that file set transientOBSWETF to false, give a OBSWETFFile, and set fixedYearOBSWETF
        ! to some valid year. If you wish to use only dynamically determined wetland fractions set transientOBSWETF
        ! to false and set fixedYearOBSWETF to -9999. The slope fractions in the init_file will then be used to
        ! dynamically determine wetland extent.
        transientOBSWETF = .false. ,  !&lt; use observed wetland fraction time series, otherwise use fixedYearOBSWETF
        OBSWETFFile = '',             !&lt; Location of the netcdf file containing observed wetland fraction
        fixedYearOBSWETF = -9999 ,    !&lt; set the year to use for observed wetland fraction if transientOBSWETF is false.
</pre><p>CLASS switches determine the configuration of the physics only as well as CLASS+CTEM (physics and biogeochemistry) runs. </p><pre class="fragment">    ! Physics switches:

        ican = 4 ,     !&lt; Number of PFTs considered by the physics subroutines. NOTE: The number specified here must match the data in your init netcdf file.
        IDISP = 0 ,    !&lt; if idisp=0, vegetation displacement heights are ignored, because the atmospheric model considers these to be part
                        !&lt; of the "terrain". if idisp=1, vegetation displacement heights are calculated.
        IZREF = 2 ,    !&lt; if izref=1, the bottom of the atmospheric model is taken lie at the ground surface.
                        !&lt; if izref=2, the bottom of the atmospheric model is taken to lie at the local roughness height.
        ISLFD = 0 ,    !&lt; if islfd=0, drcoef is called for surface stability corrections and the original gcm set of screen-level diagnostic calculations
                        !&lt; is done. if islfd=1, drcoef is called for surface stability corrections and sldiag is called for screen-level diagnostic calculations.
                        !&lt; if islfd=2, flxsurfz is called for surface stability corrections and diasurf is called for screen-level diagnostic calculations.
</pre><p>! The implications of the ISLFD switch is discussed more in <a class="el" href="CLASST_8f.html" title="Calls subroutines to perform surface energy budget calculations. ">CLASST.f</a> </p><pre class="fragment">        IPCP = 1 ,     !&lt; if ipcp=1, the rainfall-snowfall cutoff is taken to lie at 0 C. if ipcp=2, a linear partitioning of precipitation between
                        !&lt; rainfall and snowfall is done between 0 C and 2 C. if ipcp=3, rainfall and snowfall are partitioned according to
                        !&lt; a polynomial curve between 0 C and 6 C.
        IWF = 0 ,      !&lt; if iwf=0, only overland flow and baseflow are modelled, and the ground surface slope is not modelled. if iwf=n (0&lt;n&lt;4) ,
                       !&lt; the watflood calculations of overland flow and interflow are performed; interflow is drawn from the top n soil layers.
        isnoalb = 0 ,  !&lt; if isnoalb is set to 0, the original two-band snow albedo algorithms are used. if it is set to 1, the new four-band routines are used.
</pre><p>The iteration scheme for canopy or ground surface temperatures can be either a bisection or Newton-Raphson method. This is usually set to bisection </p><pre class="fragment">     ! Iteration scheme
        !&lt; ITC, ITCG and ITG are switches to choose the iteration scheme to be used in calculating the canopy or ground surface temperature
        !&lt; respectively.  if the switch is set to 1, a bisection method is used; if to 2, the newton-raphson method is used.
        ITC = 1 ,   !&lt; Canopy
        ITCG = 1 ,  !&lt; Ground under canopy
        ITG = 1 ,   !&lt; Ground
</pre><p>User supplied values can be used for plant area index, vegetation height, and canopy, soil, or snow albedos. If any of these inputs are supplied the <a class="el" href="model__state__drivers_8f90.html" title="Central driver to read in, and write out all model state variables (replacing INI and CTM files) as w...">model_state_drivers.f90</a> needs to be adapted to read-in the user-supplied values. </p><pre class="fragment">     ! User-supplied values:
        !&lt; if ipai, ihgt, ialc, ials and ialg are zero, the values of plant area index, vegetation height, canopy albedo, snow albedo
        !&lt; and soil albedo respectively calculated by class are used. if any of these switches is set to 1, the value of the
        !&lt; corresponding parameter calculated by class is overridden by a user-supplied input value.
        IPAI = 0 ,  !&lt; Plant area index
        IHGT = 0 ,  !&lt; Vegetation height
        IALC = 0 ,  !&lt; Canopy albedo
        IALS = 0 ,  !&lt; Snow albedo
        IALG = 0 ,  !&lt; Soil albedo
</pre><p>Model outputs are in netcdf format. The outputs metadata is read in from an xmlFile (see <a class="el" href="xmlSystem.html">Configuring the model outputs via the CLASSIC code and Output Variable Editor (OVE)</a>). Outputs can be grid-cell average (default), per PFT, or per tile. In all cases the output has to be specified in the xmlFile and also properly handled in <a class="el" href="prepareOutputs_8f90.html" title="Central module that handles all CTEM preparation and writing of output files. ">prepareOutputs.f90</a>. Temporal resolution of output files are half-hourly (for physics variables as well as photosynthesis and canopy conductance only), daily, monthly and annually. For half-hourly and daily outputs the start and end days as well as start and end years can be specified. Monthly file can specify the year to start writing the outputs. </p><pre class="fragment">    ! Output options:

        output_directory = 'outputFiles' ,        !&lt; Directory where the output netcdfs will be placed
        xmlFile = 'configurationFiles/outputVariableDescriptors_v1.2.xml' ,  !&lt; location of the xml file that outlines the possible netcdf output files

        doperpftoutput = .true. ,   !&lt; Switch for making extra output files that are at the per PFT level
        dopertileoutput = .false. , !&lt; Switch for making extra output files that are at the per tile level

        dohhoutput = .false. ,      !&lt; Switch for making half hourly output files (annual are always outputted)
        JHHSTD = 166 ,                !&lt; day of the year to start writing the half-hourly output
        JHHENDD = 185 ,             !&lt; day of the year to stop writing the half-hourly output
        JHHSTY = 1901 ,             !&lt; simulation year (iyear) to start writing the half-hourly output
        JHHENDY = 1901 ,            !&lt; simulation year (iyear) to stop writing the half-hourly output

        dodayoutput = .false. ,     !&lt; Switch for making daily output files (annual are always outputted)
        JDSTD = 20 ,                 !&lt; day of the year to start writing the daily output
        JDENDD = 30 ,              !&lt; day of the year to stop writing the daily output
        JDSTY = 1902 ,              !&lt; simulation year (iyear) to start writing the daily output
        JDENDY = 1903 ,             !&lt; simulation year (iyear) to stop writing the daily output

        domonthoutput = .true. ,    !&lt; Switch for making monthly output files (annual are always outputted)
        JMOSTY = 1901 ,             !&lt; Year to start writing out the monthly output files.
</pre><p>Comments can be added to output files using the Comment field below. Also comments can be left in the joboptions file after the backslash. </p><pre class="fragment">        Comment = ' test '          !&lt; Comment about the run that will be written to the output netcdfs

     /

    This area can be used for comments about the run.</pre> </div></div><!-- contents -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="navelem"><a class="el" href="index.html">CLASSIC main page</a></li>
    <li class="footer">Generated on Fri Sep 21 2018 13:07:59 for CLASSIC by
    <a href="http://www.doxygen.org/index.html">
    <img class="footer" src="doxygen.png" alt="doxygen"/></a> 1.8.13 </li>
  </ul>
</div>
</body>
</html>
